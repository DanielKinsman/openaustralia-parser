#!/usr/bin/env ruby

$:.unshift "#{File.dirname(__FILE__)}/lib"

require 'configuration'
require 'people'
require 'hansard_parser'

conf = Configuration.new

# First load people back in so that we can look up member id's
people = People.read_csv("data/members.csv")

system("mkdir -p pwdata/scrapedxml/debates")

# Required to workaround long viewstates generated by .NET (whatever that means)
# See http://code.whytheluckystiff.net/hpricot/ticket/13
Hpricot.buffer_size = 262144

agent = WWW::Mechanize.new
agent.set_proxy(conf.proxy_host, conf.proxy_port)

date = Date.new(2007, 9, 20)
url = "http://parlinfoweb.aph.gov.au/piweb/browse.aspx?path=Chamber%20%3E%20House%20Hansard%20%3E%20#{date.year}%20%3E%20#{date.day}%20#{Date::MONTHNAMES[date.month]}%20#{date.year}"
page = agent.get(url)

xml_filename = "pwdata/scrapedxml/debates/debates#{date}.xml"

HansardParser.parse_day_page(page, date, agent, people, xml_filename)

# Temporary hack: nicely indent XML
system("tidy -quiet -indent -xml -modify -wrap 0 -utf8 #{xml_filename}")

# And load up the database
system(conf.web_root + "/twfy/scripts/xml2db.pl --debates --all --force")
